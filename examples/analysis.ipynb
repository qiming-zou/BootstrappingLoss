{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of the loss function with bootstrapping\n",
    "https://arxiv.org/pdf/1412.6596\n",
    "\n",
    "Multinomial classification task with noisy labels. Loss function modifications:\n",
    "\n",
    "a) Soft version:\n",
    "\n",
    "$$\n",
    "L_{soft}(\\mathbf{q}, \\mathbf{t}) = - \\sum_{k=1}^{L} (\\beta t_{k} + (1 - \\beta) q_{k}) \\log(q_{k}),\n",
    "$$\n",
    "where $\\mathbf{q}$ is a single image class probabilities, $\\mathbf{t}$ is the ground truth, $L$ is the number of classes. Parameter $\\beta$ is chosen between $0$ and $1$. \n",
    "\n",
    "a) Hard version:\n",
    "\n",
    "$$\n",
    "L_{hard}(\\mathbf{q}, \\mathbf{t}) = - \\sum_{k=1}^{L} (\\beta t_{k} + (1 - \\beta) z_{k}) \\log(q_{k}),\n",
    "$$\n",
    "where $z_{k}$ is argmax of $\\mathbf{q}$ (similar form as $\\mathbf{t}$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "print(torch.__version__)\n",
    "\n",
    "L = 3\n",
    "\n",
    "# Some ground truth samples:\n",
    "t_1 = torch.tensor([0])\n",
    "t_2 = torch.tensor([1])\n",
    "logit_q_1a = torch.tensor([[0.9, 0.05, 0.05], ])\n",
    "logit_q_1b = torch.tensor([[0.2, 0.5, 0.3], ])\n",
    "logit_q_2a = torch.tensor([[0.33, 0.33, 0.33], ])\n",
    "logit_q_2b = torch.tensor([[0.15, 0.7, 0.15], ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Soft version\n",
    "\n",
    "Let's first compute cross entropy term: $-\\sum_{k} t_{k} \\log(q_{k})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.6178), tensor(1.2398), tensor(1.0986), tensor(0.7673))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(logit_q_1a, t_1), F.cross_entropy(logit_q_1b, t_1), F.cross_entropy(logit_q_2a, t_2), F.cross_entropy(logit_q_2b, t_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute the second term (soft bootstrapping) : $-\\sum_{k} q_{k} \\log(q_{k})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_boostrapping(logit_q):\n",
    "    return - torch.sum(F.softmax(logit_q, dim=1) * F.log_softmax(logit_q, dim=1), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.0095]), tensor([1.0906]), tensor([1.0986]), tensor([1.0619]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soft_boostrapping(logit_q_1a), soft_boostrapping(logit_q_1b), soft_boostrapping(logit_q_2a), soft_boostrapping(logit_q_2b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_bootstrapping_loss(logit_q, t, beta):\n",
    "    return F.cross_entropy(logit_q, t) * beta + (1.0 - beta) * soft_boostrapping(logit_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.6374]), tensor([1.2324]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soft_bootstrapping_loss(logit_q_1a, t_1, beta=0.95), soft_bootstrapping_loss(logit_q_1b, t_1, beta=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.0986]), tensor([0.7820]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soft_bootstrapping_loss(logit_q_2a, t_2, beta=0.95), soft_bootstrapping_loss(logit_q_2b, t_2, beta=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hard version\n",
    "\n",
    "Let's first compute cross entropy term: $-\\sum_{k} t_{k} \\log(q_{k})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.6178), tensor(1.2398), tensor(1.0986), tensor(0.7673))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(logit_q_1a, t_1), F.cross_entropy(logit_q_1b, t_1), F.cross_entropy(logit_q_2a, t_2), F.cross_entropy(logit_q_2b, t_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute the second term (hard bootstrapping) : $-\\sum_{k} z_{k} \\log(q_{k})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hard_boostrapping(logit_q):\n",
    "    _, z = torch.max(F.softmax(logit_q, dim=1), dim=1)\n",
    "    z = z.view(-1, 1)\n",
    "    return - F.log_softmax(logit_q, dim=1).gather(1, z).view(-1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.6178]), tensor([0.9398]), tensor([1.0986]), tensor([0.7673]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hard_boostrapping(logit_q_1a), hard_boostrapping(logit_q_1b), hard_boostrapping(logit_q_2a), hard_boostrapping(logit_q_2b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hard_bootstrapping_loss(logit_q, t, beta):\n",
    "    return F.cross_entropy(logit_q, t) * beta + (1.0 - beta) * hard_boostrapping(logit_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.6178]), tensor([1.1798]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hard_bootstrapping_loss(logit_q_1a, t_1, beta=0.8), hard_bootstrapping_loss(logit_q_1b, t_1, beta=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.0986]), tensor([0.7673]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hard_bootstrapping_loss(logit_q_2a, t_2, beta=0.8), hard_bootstrapping_loss(logit_q_2b, t_2, beta=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = torch.rand(4, 10, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 2, 5, 2])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = F.softmax(y_pred, dim=1).argmax(dim=1)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 2, 5, 2])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, z2 = F.softmax(y_pred, dim=1).max(dim=1)\n",
    "z2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
