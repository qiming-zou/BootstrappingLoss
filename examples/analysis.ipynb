{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of the loss function with bootstrapping\n",
    "https://arxiv.org/pdf/1412.6596\n",
    "\n",
    "Multinomial classification task with noisy labels. Loss function modifications:\n",
    "\n",
    "a) Soft version:\n",
    "\n",
    "$$\n",
    "L_{soft}(\\mathbf{q}, \\mathbf{t}) = - \\sum_{k=1}^{L} (\\beta t_{k} + (1 - \\beta) q_{k}) \\log(q_{k}),\n",
    "$$\n",
    "where $\\mathbf{q}$ is a single image class probabilities, $\\mathbf{t}$ is the ground truth, $L$ is the number of classes. Parameter $\\beta$ is chosen between $0$ and $1$. \n",
    "\n",
    "a) Hard version:\n",
    "\n",
    "$$\n",
    "L_{hard}(\\mathbf{q}, \\mathbf{t}) = - \\sum_{k=1}^{L} (\\beta t_{k} + (1 - \\beta) z_{k}) \\log(q_{k}),\n",
    "$$\n",
    "where $z_{k}$ is argmax of $\\mathbf{q}$ (similar form as $\\mathbf{t}$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "L = 3\n",
    "\n",
    "# Some ground truth samples:\n",
    "t_1 = torch.LongTensor([0])\n",
    "t_2 = torch.LongTensor([1])\n",
    "q_1a = torch.FloatTensor([[0.9, 0.05, 0.05], ])\n",
    "q_1b = torch.FloatTensor([[0.2, 0.5, 0.3], ])\n",
    "q_2a = torch.FloatTensor([[0.33, 0.33, 0.33], ])\n",
    "q_2b = torch.FloatTensor([[0.15, 0.7, 0.15], ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Soft version\n",
    "\n",
    "Let's first compute cross entropy term: $-\\sum_{k} t_{k} \\log(q_{k})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.6178), tensor(1.2398), tensor(1.0986), tensor(0.7673))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(q_1a, t_1), F.cross_entropy(q_1b, t_1), F.cross_entropy(q_2a, t_2), F.cross_entropy(q_2b, t_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute the second term (soft bootstrapping) : $-\\sum_{k} q_{k} \\log(q_{k})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_boostrapping(q):\n",
    "    return - torch.sum(F.softmax(q, dim=1) * F.log_softmax(q, dim=1), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 1.0095]), tensor([ 1.0906]), tensor([ 1.0986]), tensor([ 1.0619]))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soft_boostrapping(q_1a), soft_boostrapping(q_1b), soft_boostrapping(q_2a), soft_boostrapping(q_2b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_bootstrapping_loss(q, t, beta):\n",
    "    return F.cross_entropy(q, t) * beta + (1.0 - beta) * soft_boostrapping(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.6374]), tensor([ 1.2324]))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soft_bootstrapping_loss(q_1a, t_1, beta=0.95), soft_bootstrapping_loss(q_1b, t_1, beta=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 1.0986]), tensor([ 0.7820]))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soft_bootstrapping_loss(q_2a, t_2, beta=0.95), soft_bootstrapping_loss(q_2b, t_2, beta=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hard version\n",
    "\n",
    "Let's first compute cross entropy term: $-\\sum_{k} t_{k} \\log(q_{k})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.6178), tensor(1.2398), tensor(1.0986), tensor(0.7673))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(q_1a, t_1), F.cross_entropy(q_1b, t_1), F.cross_entropy(q_2a, t_2), F.cross_entropy(q_2b, t_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute the second term (hard bootstrapping) : $-\\sum_{k} z_{k} \\log(q_{k})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hard_boostrapping(q):\n",
    "    _, z = torch.max(F.softmax(q, dim=1), dim=1)\n",
    "    z = z.view(-1, 1)\n",
    "    return - F.log_softmax(q, dim=1).gather(1, z).view(-1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.6178]), tensor([ 0.9398]), tensor([ 1.0986]), tensor([ 0.7673]))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hard_boostrapping(q_1a), hard_boostrapping(q_1b), hard_boostrapping(q_2a), hard_boostrapping(q_2b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hard_bootstrapping_loss(q, t, beta):\n",
    "    return F.cross_entropy(q, t) * beta + (1.0 - beta) * hard_boostrapping(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.6178]), tensor([ 1.1798]))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hard_bootstrapping_loss(q_1a, t_1, beta=0.8), hard_bootstrapping_loss(q_1b, t_1, beta=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 1.0986]), tensor([ 0.7673]))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hard_bootstrapping_loss(q_2a, t_2, beta=0.8), hard_bootstrapping_loss(q_2b, t_2, beta=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
